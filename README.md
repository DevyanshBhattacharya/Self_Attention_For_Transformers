# Self_Attention_for_transformer
Implementation of one of the basic mechanisms for a fully functioning transformer module
