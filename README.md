# Self_Attention_for_transformer
Implementation of one of the basic mechanisms for a fully functioning transformer module
Transformers are a deep learning model used for natural language processing (NLP) and computer vision (CV) tasks.
They utilize a mechanism called “self-attention” to process sequential input data.
Transformers can simultaneously process all input data, capturing context and relevance.
They can handle longer sequences efficiently and overcome the vanishing gradients problem faced by recurrent neural networks (RNNs).
For more details, refer to the following research paper. https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.  
