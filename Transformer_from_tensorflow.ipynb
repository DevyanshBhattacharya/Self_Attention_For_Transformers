{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPotkO17nXCx1SegkuJA4W6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DevyanshBhattacharya/Self_Attention_for_transformer/blob/main/Transformer_from_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "g3fPrBnFkyNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "def positional_encoding(length: int, depth: int):\n",
        "    depth = depth / 2\n",
        "\n",
        "    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "    angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "    angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "AW6mxhNIr0xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, vocab_size: int, d_model: int, embedding: tf.keras.layers.Embedding=None):\n",
        "\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) if embedding is None else embedding\n",
        "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "    def compute_mask(self, *args, **kwargs):\n",
        "\n",
        "        return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        length = tf.shape(x)[1]\n",
        "        # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "        return x"
      ],
      "metadata": {
        "id": "iahoSxHLr78B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCil6bm0cYCh"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, d_model: int, num_heads: int, dff: int, dropout_rate: float=0.1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attention = GlobalSelfAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=d_model,\n",
        "            dropout=dropout_rate\n",
        "            )\n",
        "\n",
        "        self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "\n",
        "        x = self.self_attention(x)\n",
        "        x = self.ffn(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, **kwargs: dict):\n",
        "\n",
        "        super().__init__()\n",
        "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "        self.add = tf.keras.layers.Add()"
      ],
      "metadata": {
        "id": "7cu2d-I5sDM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(BaseAttention):\n",
        "\n",
        "    def call(self, x: tf.Tensor, context: tf.Tensor) -> tf.Tensor:\n",
        "\n",
        "        attn_output, attn_scores = self.mha(query=x, key=context, value=context, return_attention_scores=True)\n",
        "\n",
        "        # Cache the attention scores for plotting later.\n",
        "        self.last_attn_scores = attn_scores\n",
        "\n",
        "        x = self.add([x, attn_output])\n",
        "        x = self.layernorm(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "3NcnZQWHsFbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_vocab_size = 1000\n",
        "decoder_vocab_size = 1100\n",
        "d_model = 512\n",
        "\n",
        "encoder_embedding_layer = PositionalEmbedding(1000, d_model)\n",
        "decoder_embedding_layer = PositionalEmbedding(1000, d_model)\n",
        "\n",
        "random_encoder_input = np.random.randint(0, encoder_vocab_size, size=(1, 100))\n",
        "decoder_embedding_layer = PositionalEmbedding(1100, d_model)\n",
        "encoder_embeddings = encoder_embedding_layer(random_encoder_input)\n",
        "decoder_embeddings = decoder_embedding_layer(random_decoder_input)\n",
        "\n",
        "print(\"encoder_embeddings shape\", encoder_embeddings.shape)\n",
        "print(\"decoder_embeddings shape\", decoder_embeddings.shape)\n",
        "\n",
        "cross_attention_layer = CrossAttention(num_heads=2, key_dim=512)\n",
        "cross_attention_output = cross_attention_layer(decoder_embeddings, encoder_embeddings)\n",
        "\n",
        "print(\"cross_attention_output shape\", cross_attention_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c3V2ekgsHAr",
        "outputId": "4cf13476-11f0-474f-f2ea-67f0494550ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_embeddings shape (1, 100, 512)\n",
            "decoder_embeddings shape (1, 110, 512)\n",
            "cross_attention_output shape (1, 110, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_embeddings.shape (1, 100, 512)\n",
        "decoder_embeddings.shape (1, 110, 512)\n",
        "cross_attention_output.shape (1, 110, 512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "tq4IGB15sIl_",
        "outputId": "b8704c75-82f8-44f8-84af-390cf8581a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'TensorShape' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-2556531df901>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdecoder_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m110\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcross_attention_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m110\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'TensorShape' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GlobalSelfAttention(BaseAttention):\n",
        "\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "\n",
        "        attn_output = self.mha(query=x, value=x, key=x)\n",
        "        x = self.add([x, attn_output])\n",
        "        x = self.layernorm(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "PcM73sEcsKh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_vocab_size = 1000\n",
        "d_model = 512\n",
        "\n",
        "encoder_embedding_layer = PositionalEmbedding(1000, d_model)\n",
        "\n",
        "random_encoder_input = np.random.randint(0, encoder_vocab_size, size=(1, 100))\n",
        "\n",
        "encoder_embeddings = encoder_embedding_layer(random_encoder_input)\n",
        "\n",
        "print(\"encoder_embeddings shape\", encoder_embeddings.shape)\n",
        "\n",
        "cross_attention_layer = GlobalSelfAttention(num_heads=2, key_dim=512)\n",
        "cross_attention_output = cross_attention_layer(encoder_embeddings)\n",
        "\n",
        "print(\"global_self_attention_output shape\", cross_attention_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqR9VrGbsMHf",
        "outputId": "a9fa014d-bce1-4640-a071-e84d440da223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_embeddings shape (1, 100, 512)\n",
            "global_self_attention_output shape (1, 100, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_embeddings shape (1, 100, 512)\n",
        "global_self_attention_output shape (1, 100, 512)"
      ],
      "metadata": {
        "id": "dwzhG389sNzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "\n",
        "        attn_output = self.mha(query=x, value=x, key=x, use_causal_mask = True)\n",
        "        x = self.add([x, attn_output])\n",
        "        x = self.layernorm(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "yZGjZR_XsQ97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_vocab_size = 1100\n",
        "d_model = 512\n",
        "\n",
        "decoder_embedding_layer = PositionalEmbedding(1000, d_model)\n",
        "\n",
        "random_decoder_input = np.random.randint(0, decoder_vocab_size, size=(1, 110))\n",
        "\n",
        "decoder_embeddings = decoder_embedding_layer(random_decoder_input)\n",
        "\n",
        "print(\"decoder_embeddings shape\", decoder_embeddings.shape)\n",
        "\n",
        "causal_self_attention_layer = CausalSelfAttention(num_heads=2, key_dim=512)\n",
        "causal_self_attention_output = causal_self_attention_layer(decoder_embeddings)\n",
        "\n",
        "print(\"causal_self_attention_output shape\", causal_self_attention_output.shape)\n",
        "\n",
        "out1 = causal_self_attention_layer(decoder_embedding_layer(random_decoder_input[:, :50])) # Only the first 50 tokens beffore applying the embedding layer\n",
        "out2 = causal_self_attention_layer(decoder_embedding_layer(random_decoder_input)[:, :50]) # Only the first 50 tokens after applying the embedding layer\n",
        "\n",
        "diff = tf.reduce_max(tf.abs(out1 - out2)).numpy()\n",
        "\n",
        "print(\"Difference between the two outputs:\", diff)"
      ],
      "metadata": {
        "id": "u03FkkyrsS5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_embeddings shape (1, 110, 512)\n",
        "causal_self_attention_output shape (1, 110, 512)\n",
        "Difference between the two outputs: 0.0"
      ],
      "metadata": {
        "id": "n0Qsgn7_sUSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, d_model: int, dff: int, dropout_rate: float=0.1):\n",
        "\n",
        "        super().__init__()\n",
        "        self.seq = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model),\n",
        "            tf.keras.layers.Dropout(dropout_rate)\n",
        "        ])\n",
        "        self.add = tf.keras.layers.Add()\n",
        "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "\n",
        "        x = self.add([x, self.seq(x)])\n",
        "        x = self.layer_norm(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "vbHblyjjsWPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_vocab_size = 1000\n",
        "d_model = 512\n",
        "\n",
        "encoder_embedding_layer = PositionalEmbedding(1000, d_model)\n",
        "\n",
        "random_encoder_input = np.random.randint(0, encoder_vocab_size, size=(1, 100))\n",
        "\n",
        "encoder_embeddings = encoder_embedding_layer(random_encoder_input)\n",
        "\n",
        "print(\"encoder_embeddings shape\", encoder_embeddings.shape)\n",
        "\n",
        "feed_forward_layer = FeedForward(d_model, dff=2048)\n",
        "feed_forward_output = feed_forward_layer(encoder_embeddings)\n",
        "\n",
        "print(\"feed_forward_output shape\", feed_forward_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYrlMdhFsaKq",
        "outputId": "91ce3f74-db0c-4129-8fe5-8ae382982811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_embeddings shape (1, 100, 512)\n",
            "feed_forward_output shape (1, 100, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_vocab_size = 1000\n",
        "d_model = 512\n",
        "\n",
        "encoder_embedding_layer = PositionalEmbedding(1000, d_model)\n",
        "\n",
        "random_encoder_input = np.random.randint(0, encoder_vocab_size, size=(1, 100))\n",
        "\n",
        "encoder_embeddings = encoder_embedding_layer(random_encoder_input)\n",
        "\n",
        "print(\"encoder_embeddings shape\", encoder_embeddings.shape)\n",
        "\n",
        "encoder_layer = EncoderLayer(d_model, num_heads=2, dff=2048)\n",
        "\n",
        "encoder_layer_output = encoder_layer(encoder_embeddings)\n",
        "\n",
        "print(\"encoder_layer_output shape\", encoder_layer_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQoixrW1k-Ro",
        "outputId": "752b5407-2f52-4761-ece7-05127823886a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_embeddings shape (1, 100, 512)\n",
            "encoder_layer_output shape (1, 100, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_embeddings shape (1, 100, 512)\n",
        "encoder_layer_output shape (1, 100, 512)"
      ],
      "metadata": {
        "id": "ddfog5A3k_lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, num_layers: int, d_model: int, num_heads: int, dff: int, vocab_size: int, dropout_rate: float=0.1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "        self.enc_layers = [\n",
        "            EncoderLayer(d_model=d_model,\n",
        "                        num_heads=num_heads,\n",
        "                        dff=dff,\n",
        "                        dropout_rate=dropout_rate)\n",
        "            for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "\n",
        "        x = self.pos_embedding(x)\n",
        "        # here x has shape `(batch_size, seq_len, d_model)`\n",
        "\n",
        "        # Add dropout.\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x)\n",
        "\n",
        "        return x  # Shape `(batch_size, seq_len, d_model)`"
      ],
      "metadata": {
        "id": "DIPk3l4QlBw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_vocab_size = 1000\n",
        "d_model = 512\n",
        "\n",
        "encoder = Encoder(num_layers=2, d_model=d_model, num_heads=2, dff=2048, vocab_size=encoder_vocab_size)\n",
        "\n",
        "random_encoder_input = np.random.randint(0, encoder_vocab_size, size=(1, 100))\n",
        "\n",
        "encoder_output = encoder(random_encoder_input)\n",
        "\n",
        "print(\"random_encoder_input shape\", random_encoder_input.shape)\n",
        "print(\"encoder_output shape\", encoder_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox-jFBaolEHR",
        "outputId": "0f9cbe78-8ba5-42c3-d2f7-17516fee34ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random_encoder_input shape (1, 100)\n",
            "encoder_output shape (1, 100, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_encoder_input shape (1, 100)\n",
        "encoder_output shape (1, 100, 512)"
      ],
      "metadata": {
        "id": "QZdHNSXvlFSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, d_model: int, num_heads: int, dff: int, dropout_rate: float=0.1):\n",
        "\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.causal_self_attention = CausalSelfAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=d_model,\n",
        "            dropout=dropout_rate)\n",
        "\n",
        "        self.cross_attention = CrossAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=d_model,\n",
        "            dropout=dropout_rate)\n",
        "\n",
        "        self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "    def call(self, x: tf.Tensor, context: tf.Tensor) -> tf.Tensor:\n",
        "\n",
        "        x = self.causal_self_attention(x=x)\n",
        "        x = self.cross_attention(x=x, context=context)\n",
        "\n",
        "        # Cache the last attention scores for plotting later\n",
        "        self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "        return x"
      ],
      "metadata": {
        "id": "2LZO-udFlHRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test DecoderLayer layer\n",
        "decoder_vocab_size = 1000\n",
        "d_model = 512\n",
        "dff = 2048\n",
        "num_heads = 8\n",
        "\n",
        "decoder_layer = DecoderLayer(d_model, num_heads, dff)\n",
        "\n",
        "random_decoderLayer_input = np.random.randint(0, decoder_vocab_size, size=(1, 110))\n",
        "\n",
        "decoder_embeddings = encoder_embedding_layer(random_decoderLayer_input)\n",
        "\n",
        "decoderLayer_output = decoder_layer(decoder_embeddings, encoder_output)\n",
        "\n",
        "print(\"random_decoder_input shape\", random_decoderLayer_input.shape)\n",
        "print(\"decoder_embeddings shape\", decoder_embeddings.shape)\n",
        "print(\"decoder_output shape\", decoderLayer_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRe2bR7plIye",
        "outputId": "3c118509-f7b9-4fa2-d2cd-2ee6b378b7c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random_decoder_input shape (1, 110)\n",
            "decoder_embeddings shape (1, 110, 512)\n",
            "decoder_output shape (1, 110, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_decoder_input shape (1, 110)\n",
        "decoder_embeddings shape (1, 110, 512)\n",
        "decoder_output shape (1, 110, 512)"
      ],
      "metadata": {
        "id": "5RZvzWf5lJ_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, num_layers: int, d_model: int, num_heads: int, dff: int, vocab_size: int, dropout_rate: float=0.1):\n",
        "\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dec_layers = [\n",
        "            DecoderLayer(\n",
        "                d_model=d_model,\n",
        "                num_heads=num_heads,\n",
        "                dff=dff,\n",
        "                dropout_rate=dropout_rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.last_attn_scores = None\n",
        "\n",
        "    def call(self, x: tf.Tensor, context: tf.Tensor) -> tf.Tensor:\n",
        "\n",
        "        # `x` is token-IDs shape (batch, target_seq_len)\n",
        "        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x  = self.dec_layers[i](x, context)\n",
        "\n",
        "        self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "        # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "        return x"
      ],
      "metadata": {
        "id": "LEcp9xq9lPTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test decoder layer\n",
        "decoder_vocab_size = 1000\n",
        "d_model = 512\n",
        "\n",
        "decoder_layer = Decoder(num_layers=2, d_model=d_model, num_heads=2, dff=2048, vocab_size=decoder_vocab_size)\n",
        "\n",
        "random_decoder_input = np.random.randint(0, decoder_vocab_size, size=(1, 100))\n",
        "\n",
        "decoder_output = decoder_layer(random_decoder_input, encoder_output)\n",
        "\n",
        "print(\"random_decoder_input shape\", random_decoder_input.shape)\n",
        "print(\"decoder_output shape\", decoder_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_skHhlTKlRDW",
        "outputId": "b78fa58d-7d54-4e3b-cfdd-54fa7f4a63e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random_decoder_input shape (1, 100)\n",
            "decoder_output shape (1, 100, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_decoder_input shape (1, 100)\n",
        "decoder_output shape (1, 100, 512)"
      ],
      "metadata": {
        "id": "ZrcGouUvlR_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Transformer(\n",
        "    input_vocab_size: int,\n",
        "    target_vocab_size: int,\n",
        "    encoder_input_size: int = None,\n",
        "    decoder_input_size: int = None,\n",
        "    num_layers: int=6,\n",
        "    d_model: int=512,\n",
        "    num_heads: int=8,\n",
        "    dff: int=2048,\n",
        "    dropout_rate: float=0.1,\n",
        "    ) -> tf.keras.Model:\n",
        "\n",
        "    inputs = [\n",
        "        tf.keras.layers.Input(shape=(encoder_input_size,), dtype=tf.int64),\n",
        "        tf.keras.layers.Input(shape=(decoder_input_size,), dtype=tf.int64)\n",
        "        ]\n",
        "\n",
        "    encoder_input, decoder_input = inputs\n",
        "\n",
        "    encoder = Encoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff, vocab_size=input_vocab_size, dropout_rate=dropout_rate)(encoder_input)\n",
        "    decoder = Decoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff, vocab_size=target_vocab_size, dropout_rate=dropout_rate)(decoder_input, encoder)\n",
        "\n",
        "    output = tf.keras.layers.Dense(target_vocab_size)(decoder)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=output)"
      ],
      "metadata": {
        "id": "yzhXJc_ilUQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_size = 100\n",
        "decoder_input_size = 110\n",
        "\n",
        "encoder_vocab_size = 1000\n",
        "decoder_vocab_size = 1000\n",
        "\n",
        "model = Transformer(\n",
        "    input_vocab_size=encoder_vocab_size,\n",
        "    target_vocab_size=decoder_vocab_size,\n",
        "    encoder_input_size=encoder_input_size,\n",
        "    decoder_input_size=decoder_input_size,\n",
        "    num_layers=2,\n",
        "    d_model=512,\n",
        "    num_heads=2,\n",
        "    dff=512,\n",
        "    dropout_rate=0.1)\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9QAS0h8lW_Q",
        "outputId": "e2c204f9-a6ec-420f-c9cb-da329bc97e8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 100)]                0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 110)]                0         []                            \n",
            "                                                                                                  \n",
            " encoder_1 (Encoder)         (None, 100, 512)             5768192   ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " decoder_1 (Decoder)         (None, 110, 512)             9971712   ['input_2[0][0]',             \n",
            "                                                                     'encoder_1[0][0]']           \n",
            "                                                                                                  \n",
            " dense_22 (Dense)            (None, 110, 1000)            513000    ['decoder_1[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 16252904 (62.00 MB)\n",
            "Trainable params: 16252904 (62.00 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}